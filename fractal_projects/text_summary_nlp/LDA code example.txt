##########Data Prep for Input to LDA: column 'text_split_ngrams' would be created that is used to create LDA model (if not exists) and apply LDA model to generate 30 topic columns############################

import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import pandas as pd
import numpy as np
from nltk.tokenize import sent_tokenize, word_tokenize
import re
from nltk.stem import PorterStemmer
from nltk.tag import pos_tag
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
import gensim
from gensim import corpora
from gensim.models import Phrases
from gensim.models import CoherenceModel
from nltk.stem import PorterStemmer

# Plotting tools
#import pyLDAvis
#import pyLDAvis.gensim  # don't skip this
#import matplotlib.pyplot as plt
#get_ipython().magic('matplotlib inline')


min_bigram_count  = 6
topic_count = 30

# Lemmatization definition & function
wnl = WordNetLemmatizer()

# this is need for the below:	 nltk.download('maxnet_treebank_pos_tagger')
def get_wordnet_pos(treebank_tag):
    '''Treebank to wordnet POS tag'''
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return 'n' #basecase POS

def lemmatize(example):
    token_tag = pos_tag(example.split())
    return(' '.join([wnl.lemmatize(w,pos=get_wordnet_pos(t)) for w,t in token_tag]))


spark.conf.set("spark.debug.maxToStringFields", 10000) 
raw_data  =spark.sql("select * from fractal_prdstg_tbls.callers_chat_input_lda where most_probable_session_intent= 'pay'")

df = raw_data.toPandas()

# In[23]:
#stopset = set(stopwords.words('english'))
stopset = set(['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don','should','now'])
stopset.update(['jessica', 'charlie','dont', 'thats', 'cant' ,'hello','happy', 'help', 'welcome', 'pleasure', 'today','thank','thanks', 'please','would', 'verizon','chat', 'want', 'like'                , 'have', 'great','much','who','what', 'okay', 'need','think','good', 'xxxx', 'lol','question','about', 'regard',               'a','about','above','across','after','again','against','all','almost','alone','along','already','also','although'                ,'always','among','an','and','another','any','anybody','anyone','anything','anywhere','are','area','areas',                'around','as','ask','asked','asking','asks','at','away','b','back','backed','backing','backs','be','because',                'become','becomes','became','been','before','began','behind','being','beings','best','better','between','big',                'both','but','by','c','came','can','cannot','case','cases','certain','certainly','clear','clearly','come','could',                'd','did','differ','different','differently','do','does','done','down','downed','downing','downs','during','e',                'each','early','either','end','ended','ending','ends','enough','even','evenly','ever','every','everybody','everyone'                ,'everything','everywhere','f','face','faces','fact','facts','far','felt','few','find','finds','first','for','four',                'from','full','fully','further','furthered','furthering','furthers','g','gave','general','generally','get','gets',                'give','given','gives','go','going','good','goods','got','great','greater','greatest','group','grouped','grouping',                'groups','h','had','has','have','having','he','her','herself','here','high','higher','highest','him','himself','his',                'how','however','i','if','important','in','interest','interested','interesting','interests','into','is','it','its',                'itself','j','just','k','keep','keeps','kind','knew','know','known','knows','l','large','largely','last','later',                'latest','least','less','let','lets','like','likely','long','longer','longest','m','made','make','making','man',                'many','may','me','member','members','men','might','more','most','mostly','mr','mrs','much','must','my','myself','n',                'necessary','need','needed','needing','needs','never','new','newer','newest','next','no','non','not','nobody','noone',
                'nothing','now','nowhere','number','numbers','o','of','off','often','old','older','oldest','on','once','one','only',\
                'open','opened','opening','opens','or','order','ordered','ordering','orders','other','others','our','out','over','p',\
                'part','parted','parting','parts','per','perhaps','place','places','point','pointed','pointing','points','possible',\
                'present','presented','presenting','presents','problem','problems','put','puts','q','quite','r','rather','really',\
                'right','room','rooms','s','said','same','saw','say','says','second','seconds','see','sees','seem','seemed','seeming',\
                'seems','several','shall','she','should','show','showed','showing','shows','side','sides','since','small','smaller','smallest','so','some','somebody','someone','something','somewhere','state','states','still','such','sure','t','take','taken','than','that','the','their','them','then','there','therefore','these','they','thing','things','think','thinks','this','those','though','thought','thoughts','three','through','thus','to','today','together','too','took','toward','turn','turned','turning','turns','two','u','under','until','up','upon','us','use','uses','used','v','very','w','want','wanted','wanting','wants','was','way','ways','we','well','wells','went','were','what','when','where','whether','which','while','who','whole','whose','why','will','with','within','without','work','worked','working','works','would','y','year','years','yet','you','young','younger','youngest','your','yours','hello','sorry','tell','try','suppose','karen','jessica','mike','mark','guy','hope', 'cool', 'josh','stacy', 'nope', 'thankyou','dave','martin', 'justin', 'yeah', 'appreciate','sure'
])


# In[100]:
column = 'customer_chat_text'

def text_prep(df,column):
    df[column+'_regex'] = df[column].apply(lambda x: re.sub(r'\b\w{1,3}\b', '',re.sub('<>',' ',re.sub('><','',re.sub(' ','<>',re.sub('[^a-zA-Z]',' ',str(x)))))))
    df[column+'_lower'] = df[column+'_regex'].apply(lambda x: x.lower() )
    df[column+'_lemma'] = df[column+'_lower'].apply(lambda x: lemmatize(x))
    #df[column+'_lemma'] = df[column+'_lower'].apply(lambda x: stem(x))
    df[column+'_text_split'] = df[column+'_lemma'].apply(lambda x : x.split())
    df[column+'_text_split_no_stop'] = df[column+'_text_split'].apply(lambda x : [word for word in x if word not in stopset])


# In[101]:


text_prep(df, 'customer_chat_text')
cust_bigrams = Phrases(df[column+'_text_split_no_stop'], min_count = min_bigram_count)


df['text_split_ngrams'] = df[column+'_text_split_no_stop'].apply(lambda x: cust_bigrams[x])
df = df.drop([column+'_regex', column+'_lower',column+'_lemma', column+'_text_split',column+'_text_split_no_stop' ], axis = 1)

###########################RUN ONLY IF YOU NEED A NEW LDA MODEL############################

# Creating the term dictionary of our courpus, where every unique term is assigned an index. 
dictionary = corpora.Dictionary(df['text_split_ngrams'])

# Filter terms which occurs in less than 4 articles & more than 70% of the articles 
dictionary.filter_extremes(no_below=10, no_above=0.90)

corpus = [dictionary.doc2bow(text) for text in df['text_split_ngrams']]
# Creating the object for LDA model using gensim library
Lda = gensim.models.ldamodel.LdaModel
Lda = gensim.models.ldamulticore.LdaMulticore

# Running and Trainign LDA model on the document term matrix.
ldamodel = Lda(corpus, num_topics=topic_count, id2word = dictionary, passes=50, workers = 3)
dictionary.save('dictionary_surgecallers_propensity_sept_chat_pay')
ldamodel.save('lda_surgecallers_propensity_sept_chat_pay.model')
#dictionary.save('dictionary_surgecallers_aug2018_chat')
#ldamodel.save('lda_surgecallers_aug2018_chat.model')

topic_df = pd.DataFrame( data = {}, columns = ['topic', 'topic_contents'])
for i,topic in enumerate(ldamodel.print_topics(num_topics=int(topic_count), num_words=10)):
	words = topic[1].split("+")
	topic_df = topic_df.append({'topic': 'topic'+str(i),'topic_contents': words}, ignore_index = True)
	

topic_spark_df = spark.createDataFrame(topic_df)
topic_spark_df.createOrReplaceTempView("topic_temp_view")

spark.sql("create table fractal_prdstg_tbls.surge_callers_lda_chat_topic_pay as select * from topic_temp_view")